# ğŸ“Š Datalake Project â€“ Pipeline de DonnÃ©es Big Data

Ce projet met en place un pipeline **Data Lake** complet en local avec **Spark**, **Airflow**, et une **API Flask** pour exposer les donnÃ©es finales. Il suit lâ€™architecture classique **Bronze â†’ Silver â†’ Gold**.

---

## ğŸ“ Structure du Projet

datalake_project/
â”œâ”€â”€ Data_lake/
â”‚ â”œâ”€â”€ raw/ # DonnÃ©es brutes (ad_stream, social_media, web_logs)
â”‚ â”œâ”€â”€ silver/ # DonnÃ©es transformÃ©es
â”‚ â””â”€â”€ gold/ # Dataset final (agrÃ©gation)
â”œâ”€â”€ ingestion/
â”‚ â””â”€â”€ read_ad_stream.py
â”œâ”€â”€ transformation/
â”‚ â”œâ”€â”€ transform_ad_campaigns.py
â”‚ â”œâ”€â”€ transform_social_media.py
â”‚ â”œâ”€â”€ transform_web_logs.py
â”‚ â”œâ”€â”€ transform_gold.py
â”‚ â””â”€â”€ run_all_transformations.py
â”œâ”€â”€ api/
â”‚ â””â”€â”€ app.py # API Flask pour exposer les donnÃ©es finales
â”œâ”€â”€ airflow_env/
â”‚ â””â”€â”€ dags/
â”‚ â””â”€â”€ datalake_pipeline.py # DAG Airflow
â”œâ”€â”€ run_pipeline.sh # Script shell pour tout lancer automatiquement
â””â”€â”€ README.md

---

## âœ… Ã‰tapes du Pipeline (Relance le stream, ExÃ©cute les transformations, Met Ã  jour le dataset final, Lance lâ€™API Flask)

### 1. âš™ï¸ Setup environnement local
- Installation de Spark, Airflow, Flask
- ExÃ©cution possible avec un seul script : `run_pipeline.sh`

### 2. ğŸ“¥ Ingestion
- `read_ad_stream.py` simule un flux de donnÃ©es JSON ligne par ligne dans `raw/ad_stream`

### 3. ğŸ—ƒï¸ Zone Bronze (raw)
- DonnÃ©es stockÃ©es dans `Data_lake/raw/` : ad_stream, social_media, web_logs

### 4. ğŸ” Transformation (Silver)
- Nettoyage / structuration avec Spark :
  - `transform_ad_campaigns.py`
  - `transform_social_media.py`
  - `transform_web_logs.py`

### 5. ğŸ§± ModÃ©lisation finale (Gold)
- `transform_gold.py` fusionne les donnÃ©es Silver avec une `join_key`
- RÃ©sultat Ã©crit dans `Data_lake/gold/final_dataset`

### 6. ğŸš€ API Flask
- Expose les donnÃ©es finales via `api/app.py`
- Endpoint : `http://localhost:5000/data`

### 7. ğŸ“Š Orchestration avec Airflow
- DAG dans `airflow_env/dags/datalake_pipeline.py`
- ExÃ©cute automatiquement : ingestion â†’ transformation â†’ export

---

## ğŸ” Lancement AutomatisÃ©

Depuis le dossier `datalake_project` :

```bash
bash run_pipeline.sh


ğŸŒ Utilisation de lâ€™API
cd api
python3 app.py
Puis visite : http://localhost:5000/data

ğŸ› ï¸ Technologies
Python 3

PySpark

Pandas

Airflow

Flask

JSON / Parquet

VS Code + Terminal WSL

âœï¸ RÃ©alisÃ© par : Ossama LOURIDI & Manel ZERGUIT
