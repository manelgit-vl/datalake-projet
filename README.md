# ğŸ§  Projet Datalake Local (sans HDFS)

Ce projet met en place un pipeline complet de donnÃ©es en local, sans dÃ©pendre de Hadoop (HDFS), en utilisant **Apache Spark**, **Airflow**, et une **API Flask**. Il simule un scÃ©nario rÃ©el de traitement de donnÃ©es marketing.

---

## ğŸš€ Objectif

Traiter et exposer via une API des donnÃ©es issues de plusieurs sources marketing : campagnes publicitaires, rÃ©seaux sociaux, logs web.

---

## ğŸ“ Structure du projet

```
datalake_project/
â”œâ”€â”€ Data_lake/
â”‚   â”œâ”€â”€ raw/           â†’ DonnÃ©es brutes (JSON)
â”‚   â”œâ”€â”€ silver/        â†’ DonnÃ©es nettoyÃ©es (parquet)
â”‚   â””â”€â”€ gold/          â†’ DonnÃ©es finales combinÃ©es (parquet)
â”‚
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ read_ad_stream.py
â”œâ”€â”€ transformation/
â”‚   â”œâ”€â”€ transform_ad_campaigns.py
â”‚   â”œâ”€â”€ transform_social_media.py
â”‚   â”œâ”€â”€ transform_web_logs.py
â”‚   â”œâ”€â”€ transform_gold.py
â”‚   â””â”€â”€ run_all_transformations.py
â”œâ”€â”€ api/
â”‚   â””â”€â”€ app.py             â†’ API Flask pour exposer les donnÃ©es finales
â”œâ”€â”€ airflow_env/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ datalake_pipeline.py â†’ DAG Airflow
â”œâ”€â”€ start_datalake.sh      â†’ Script shell pour tout lancer automatiquement
â”œâ”€â”€ docker-compose.yml     â†’ (optionnel, non utilisÃ© ici)
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Ã‰tapes humaines du projet

### 1. ğŸ” Ingestion

Les fichiers JSON sont simulÃ©s comme des flux de donnÃ©es dans `Data_lake/raw`. L'ingestion est faite manuellement via des scripts Python (pas de Kafka rÃ©el).

### 2. ğŸ§¹ Transformation

Chaque type de donnÃ©e (ad campaigns, social media, web logs) est transformÃ© et nettoyÃ© avec **PySpark**, puis Ã©crit au format **parquet** dans `Data_lake/silver`.

### 3. ğŸ”— AgrÃ©gation (gold layer)

Les donnÃ©es sont combinÃ©es avec une stratÃ©gie personnalisÃ©e : comme aucune colonne n'Ã©tait commune entre les 3 sources, une clÃ© factice `join_key` a Ã©tÃ© ajoutÃ©e pour effectuer une **jointure croisÃ©e** et crÃ©er un jeu de donnÃ©es complet.

### 4. ğŸŒ API Flask

Une **API Flask** a Ã©tÃ© crÃ©Ã©e pour exposer les donnÃ©es finales (`gold/final_dataset`) au format JSON via l'URL `http://localhost:5000/data`.

### 5. ğŸ“Š Orchestration avec Airflow

Un DAG Airflow (`datalake_pipeline`) orchestre automatiquement les Ã©tapes : ingestion â†’ transformation â†’ export.

### 6. â–¶ï¸ Lancement automatisÃ©

Un **script shell unique** permet de tout lancer automatiquement en une ligne (`./start_datalake.sh`).

---

## âœ… Lancer le projet automatiquement

```bash
bash run_pipeline.sh
```

**Cela va :**

* Activer l'environnement Python
* Lancer les scripts de transformation
* DÃ©marrer l'API Flask
* DÃ©marrer Airflow webserver + scheduler

---

## ğŸŒ Utilisation de lâ€™API

```bash
cd api
python3 app.py
```

Puis ouvre ton navigateur et visite : [http://localhost:5000/data](http://localhost:5000/data)

---

## ğŸ› ï¸ Technologies

* **Python 3**
* **PySpark**
* **Pandas**
* **Airflow**
* **Flask**
* **JSON / Parquet**
* **VS Code + Terminal WSL**

---

## ğŸ“¸ Pour le rendu final

Fais des **captures dâ€™Ã©cran** de :

* Lâ€™exÃ©cution des scripts de transformation dans le terminal
* Le rÃ©sultat JSON dans le navigateur (`http://localhost:5000/data`)
* Lâ€™interface Airflow avec le DAG en succÃ¨s
* La structure du dossier dans VS Code

---

##  Auteur : Ossama Louridi & Manel ZERGUIT 
---


